---
title: "Drivers of sample number"
author: "Engagement team - Freshawater Hackathon"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  word_document: default
  html_document:
    toc: yes
    toc_float: yes
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = FALSE)
```

# Introduction

We want to know what the drivers of more samples is. We think this might be driven by things like the type of training they got, the amount and types of social engagement, length of involvement, etc.

# Sample effort analysis

The data were provided in .csv format by Ian Thornhill (Earthwatch) on the 19th June 2017 to the engagement team of the hackathon. 

I read this in and have a look at the data fixing any of the columns that need to be fixed

```{r libraries}
rm(list = ls())
library(lme4)
library(ggplot2)
library(reshape2)
library(glmm)
library(effects)
```

## Clean data

```{r data_exploration}
raw_data <- read.csv('data/HWPEngage_3_170620.csv', stringsAsFactors = FALSE)

clean_data <- raw_data

# First let's fix the date columns
for(i in c('Date','created', 'Latest')){
  clean_data[,i] <- as.Date(as.character(raw_data[,i]), format = '%d/%m/%Y')
}  

# Make country a factor
clean_data$Country <- as.factor(clean_data$Country)

# Make particip a number (note the #NUM! become NA)
clean_data$Particip <- as.numeric(clean_data$Particip)

# Let's look for NAs
nrow(clean_data)
summary(clean_data)

# We need to remove people who never record a sample
sum(clean_data$Sample == 0)
```

There are quite a few NAs but these don't appear to be in columns that we really need. They are in the `Date` column (not sure what this date is) and they are in the observed water quality colunms (`WQx`), which we dont use currently.

## Metrics

### Create social metrics

We created two different social metrics which quantified the user engagement on the website. The first metric is the communication score which is a combination of Blog, Comment, Invite, Presentation, and Share. To combine these we first devided the values (ie number of blog posts) by the number of weeks the user have been involved in the project, this removes the effect of time which will be in the model already. We then standardised these values so that they could be combined. This was done by Ian and the results are in the `XTimeZ` columns. I thought it would be a good idea to do a PCA to see if that creates a powerful PCA1.

```{r calculate_social_metrics1}
PCA_c <- prcomp(~ log(BlogTime+0.1) + log(CommTime+0.1) + log(InvTime+0.1) + log(ShareTime+0.1) + log(PresTime+0.1),
                data = clean_data,
                center = TRUE,
                scale. = TRUE)

# The first axis explains 93% of the variance
summary(PCA_c)

# This is because the values are highly correlated
biplot(PCA_c)
```

A PCA does not work here as there is not a strong correlation between the social factors. This does not stop us from summing across these values to get the total communication engagement

```{r calculate_social_metrics2}
# These numbers are scaled to have a mean of 0 but actually I want them to have a minimum of 0
# and max of 1

range01 <- function(x, ...){(x - min(x, ...)) / (max(x, ...) - min(x, ...))}

for(i in c('BlogTimeZ','CommTimeZ','InvTimeZ','ShareTimeZ','PresTimeZ')){
  
  clean_data[,i] <- range01(clean_data[,i])
  # hist(clean_data[,i], breaks = 100)
  
}

comm_score <- rowSums(clean_data[,c('BlogTimeZ','CommTimeZ','InvTimeZ','ShareTimeZ','PresTimeZ')])

clean_data$comm_score <- comm_score

# This metric has a big skew
hist(log(comm_score + 1), breaks = 50)

# We can turn in into a catagorical
# Those with 0 (n= 1510), and then the lower and upper 50% for the rest (n=~215 each)
comm_cat <- comm_score
cat50 <- quantile(x = comm_score[comm_score > 0], probs = 0.5)
comm_cat[comm_cat > cat50] <- 2
comm_cat[comm_cat <= cat50 & comm_cat > 0] <- 1
table(comm_cat)
clean_data$comm_cat <- comm_cat
```

### Sampling period

A note on sampling period. This is the time from the first activity in the project to the last sample collected and can be viewed as the sampling period. This was calculated by Ian.

### Country

We want to account for country in our model as it might be that people in one country sample more than another. This is not something we are interested in but is something we want to account for so we will keep it in the model as a random effect [this is no longer the case as I have to change to a model type that does not allow random effects, it is now included asa  fixed effect]. The data was generated by Ian by taking the country that each user was trained in. Almost all users sample in a single country, which is the country that are trained in.

### Difficulty score

The difficulty score is the first axis of an PCA on a number of different metric that quantify the complexity of the tasks users do. 

```{r pca_difficulty}
# Let's do a pca of the difficulty metrics 
PCA_d <- prcomp(~ Time + Bulk + Complex + Task,
             data = clean_data,
             center = TRUE,
             scale. = TRUE)

# The first axis explains 94% of the variance
summary(PCA_d)

# This is because the values are highly correlated
biplot(PCA_d)

# add to our data
clean_data$difficulty_score <- PCA_d$x[,1]
```

It is important to note here that points are clustered together, this is because each protocol has a different difficulty but there are only 28 different protocols, and it look like some must share the same difficulty score. This is important to think about as the location and difficultly are likely to be correlated as a result.

### Group size

We are interested whether being in a larger group when you go out sampling means you are more likely to sample more due to a social factor. This is captured in the Particp variable.

### Looking at data for model

Staff have already been removed from this dataset (that is why there is no staff column this time). 

```{r first_stats}
# At this point remove 0 samples
sample_data <- clean_data[clean_data$Sample > 0, ]

# How much data do we have for the covariates of interest?
nrow(sample_data)
summary(sample_data[, c('PeriodSamp', 'Paid', 'difficulty_score', 'comm_score', 'Team', 'Rainfall', 'Temp', 'Country', 'Particip')])
# Could we add in the WQx variables? there are onl a handfull of NAs

# We might be best logging some of our predictor variables
hist(sample_data$Particip)
hist(sample_data$PeriodSamp)
hist(sample_data$Paid)
hist(sample_data$difficulty_score)
# hist(log(sample_data$difficulty_score))
# hist(log(sample_data$comm_score + 1))
hist(sample_data$comm_cat)
hist(sample_data$QuizTime)
hist(sample_data$Rainfall)
hist(sample_data$Temp)
hist(sample_data$Sample)
hist(log(sample_data$Sample))
```

We can use a poisson distribution to account for the skew in the samples counts. We need to log the sample counts for this to work but this result in non-integer values which wont work with poission, instead we need to use quasi-poisson. In turn this means we cannot use a mixed effects model so we need to include country as a fixed efect.

```{r modelling}
# lets just add in country as a fixed effect here
m2a <- glm(log(Sample) ~ PeriodSamp + 
                        Paid +
                        Particip +
                        difficulty_score +
                        comm_cat +
                        QuizTime +
                        # Team + # this has no effect on the model but fixes UK issue
                        # Rainfall + # I dont these climate variables should be here
                        # Temp + # do we really expect them to have an effect here?
                        Country,
            data = sample_data,
            family = quasipoisson)
plot(m2a, ask = FALSE)
# the three outliers without log(samples) are the three people with the most samples
# We have 'zero-inflation' I think
# Negative binomial is not better
# zeroinflation model doesnt help
summary(m2a)

# I think that country correlates with the other variables
# which might cause issues
car::vif(m2a)
# This all looks okay

## I think this is the model we want to go with 
deviance_explained <- (m2a$null.deviance - m2a$deviance) / m2a$null.deviance 
deviance_explained

# Let's plot the significant results.
# The y-axis can be viewed as the increase in number of samples
for(i in c('PeriodSamp','Particip','difficulty_score','comm_cat','QuizTime','Country')){
  print(plot(Effect(i, m2a)))
}
```

# Training retention model

We know that a lot of people who do the training do not go on to submit any records. Why is the case? are there some attibutes of the training that these people undertake, or the people themselves, which predicts retention? Here we use a binomial modelt to see which variables predict retention (0/1 - did they submit 0 records/did they submit >0 records). There is a 6 month period from the last training event to the date of data extraction, we consider this to be a long enough period for our binomial response to be accurate.

## Data

```{r data_binomial}
# The data we want is the same as above but we want to keep people with 0 records
str(clean_data)

# Let's create our binary column
clean_data$Sample01 <- ifelse(clean_data$Sample > 0, 1, 0)

# Lets veiw the metrics we are going to use
summary(clean_data[,c('Paid','TranTrai','Upload','TrainTeam','TeamPoints','Team','Temp','Rainfall', 'People')])
cor(clean_data[,c('Paid','TranTrai','Upload','TrainTeam','TeamPoints','Team','Temp','Rainfall', 'People')])

# Take a look at some of these
hist(clean_data$TeamPoints)

# A lot of people attend training where nobody has submitted any records
sum(clean_data$TeamPoints == 0)

hist(clean_data$TrainTeam)
```

Now lets do some modelling

```{r retention_modelling}
b1 <- glm(Sample01 ~ Paid + 
                     TranTrai +
                     Upload +
                     TrainTeam +
                     log(TeamPoints+0.001) +
                     Team +
                     Temp +
                     Rainfall,
          family = binomial(link = "logit"),
          data = clean_data)
# plot(b1, ask = FALSE)
summary(b1)
step(b1)

deviance_explained <- (b1$null.deviance - b1$deviance) / b1$null.deviance 
deviance_explained

# Colinarity does not seem to be a big issue
car::vif(b1)

# Let's plot the significant results.
# The y-axis can be viewed as probability of retention after training
for(i in c('Paid','TranTrai','Upload','TrainTeam','TeamPoints','Team','Temp')){
  print(plot(Effect(i, b1)))
}
```

The results here are essentially the same as those we got on the hack day.
