---
title: "Drivers of sample number"
author: "Engagement team - Freshawater Hackathon"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  word_document: default
  html_document: default
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = FALSE)
```

# Introduction

We want to know what the drivers of more samples is. We think this might be driven by things like the type of training they got, the amount and types of social engagement, length of involvement, etc.

# Data

The data were provided in .csv format by Ian Thornhill (Earthwatch) on the 19th June 2017 to the engagement team of the hackathon. 

I read this in and have a look at the data fixing any of the columns that need to be fixed

```{r libraries}
rm(list = ls())
library(lme4)
library(ggplot2)
library(reshape2)
# library(pscl)
library(glmm)
library(sjPlot)
# library(sjmisc)
```

## Clean data

```{r data_exploration}
raw_data <- read.csv('data/HWPEngage_2_170619.csv', stringsAsFactors = FALSE)

summary(raw_data)

clean_data <- raw_data

# First let's fix the date columns
for(i in c('TrainingDate','created', 'Latest')){
  clean_data[,i] <- as.Date(as.character(raw_data[,i]), format = '%d/%m/%Y')
}  

# Make country a factor
clean_data$Country <- as.factor(clean_data$Country)

# Let's look for NAs
nrow(clean_data)
summary(clean_data)

# We need to remove people who never record a sample
sum(clean_data$Sample == 0)

clean_data <- clean_data[clean_data$Sample > 0, ]
```

There are quite a few NAs but these don't appear to be in columns that we really need. They are in the `Date` column (not sure what this date is) and they are in the observed water quality colunms (`WQx`), which we dont use currently.

# Metrics

## Create social metrics

We created two different social metrics which quantified the user engagement on the website. The first metric is the communication score which is a combination of Blog, Comment, Invite, Presentation, and Share. To combine these we first devided the values (ie number of blog posts) by the number of weeks the user have been involved in the project, this removes the effect of time which will be in the model already. We then standardised these values so that they could be combined. This was done by Ian and the results are in the `XTimeZ` columns. I thought it would be a good idea to do a PCA to see if that creates a powerful PCA1.

```{r calculate_social_metrics1}
PCA_c <- prcomp(~ log(BlogTime+0.1) + log(CommTime+0.1) + log(InvTime+0.1) + log(ShareTime+0.1) + log(PresTime+0.1),
             data = clean_data,
             center = TRUE,
             scale. = TRUE)

# The first axis explains 93% of the variance
summary(PCA_c)

# This is because the values are highly correlated
biplot(PCA_c)
```

A PCA does not work here as there is not a strong correlation between the social factors. This does not stop us from summing across these values to get the total communication engagement

```{r calculate_social_metrics1}
# These numbers are scaled to have a mean of 0 but actually I want them to have a minimum of 0
# and max of 1

range01 <- function(x, ...){(x - min(x, ...)) / (max(x, ...) - min(x, ...))}

for(i in c('BlogTimeZ','CommTimeZ','InvTimeZ','ShareTimeZ','PresTimeZ')){
  
  clean_data[,i] <- range01(clean_data[,i])
  # hist(clean_data[,i], breaks = 100)
  
}

comm_score <- rowSums(clean_data[,c('BlogTimeZ','CommTimeZ','InvTimeZ','ShareTimeZ','PresTimeZ')])

clean_data$comm_score <- comm_score

# This metric has a big skew
hist(log(comm_score + 1), breaks = 50)

# We can turn in into a catagorical
# Those with 0 (n= 1510), and then the lower and upper 50% for the rest (n=~215 each)
comm_cat <- comm_score
cat50 <- quantile(x = comm_score[comm_score > 0], probs = 0.5)
comm_cat[comm_cat > cat50] <- 2
comm_cat[comm_cat <= cat50 & comm_cat > 0] <- 1
table(comm_cat)
clean_data$comm_cat <- comm_cat
```

## Sampling period

A note on sampling period. This is the time from the first activity in the project to the last sample collected and can be viewed as the sampling period. This was calculated by Ian.

## Country

We want to account for country in our model as it might be that people in one country sample more than another. This is not something we are interested in but is something we want to account for so we will keep it in the model as a random effect. The data was generated by Ian by taking the country that each user was trained in. Almost all users sample in a single country, which is the country that are trained in.

## Difficulty score

The difficulty score is the first axis of an PCA on a number of different metric that quantify the complexity of the tasks users do. 

```{r pca_difficulty}
# Let's do a pca of the difficulty metrics 
PCA_d <- prcomp(~ Time + Bulk + Complex + Task,
             data = clean_data,
             center = TRUE,
             scale. = TRUE)

# The first axis explains 94% of the variance
summary(PCA_d)

# This is because the values are highly correlated
biplot(PCA_d)

# add to our data
clean_data$difficulty_score <- PCA_d$x[,1]
```

It is important to note here that points are clustered together, this is because each protocol has a different difficulty but there are only 28 different protocols, and it look like some must share the same difficulty score. This is important to think about as the location and difficultly are likely to be correlated as a result.

## Looking at data for model

Staff have already been removed from this dataset (that is why there is no staff column this time). 

```{r first_stats}
# How much data do we have for the covariates of interest?
nrow(clean_data)
summary(clean_data[, c('PeriodSamp', 'Paid', 'difficulty_score', 'comm_score', 'Team', 'Rainfall', 'Temp', 'Country')])
# We are going to lose lots of data through NAs in the Paid column
# WQS would be great but there are so many NAs we just can't use it

# We might be best logging some of our predictor variables
hist(clean_data$PeriodSamp)
hist(clean_data$Paid)
hist(clean_data$difficulty_score)
# hist(log(clean_data$difficulty_score))
# hist(log(clean_data$comm_score + 1))
hist(clean_data$comm_cat)
hist(clean_data$QuizTime)
hist(clean_data$Rainfall)
hist(clean_data$Temp)
hist(clean_data$Sample)
hist(log(clean_data$Sample))
```

We can use a poisson distribution to account for the skew in the samples counts. The explanatory variables are in some cases improved by logging.

```{r modelling}
# # We need to move to a mixed effects model because we want to put country in there
# m1 <- glmer(Sample ~ scale(PeriodSamp) + 
#                           Paid + 
#                           difficulty_score + 
#                           comm_cat + 
#                           QuizTime + 
#                           Team +
#                           Rainfall +
#                           Temp +
#                           (1|Country),
#             data = clean_data,
#             family = 'poisson')
# plot(m1)
# # I  dont think this model copes with the serious overdispersion of the data
# 
# # A standard glm with quasipoisson looks okay
# m2 <- glm(log(Sample) ~ scale(PeriodSamp) + 
#                         Paid + 
#                         difficulty_score + 
#                         comm_cat + 
#                         QuizTime + 
#                         Team +
#                         Rainfall +
#                         Temp, 
#                         # (1|Country),
#             data = clean_data,
#             family = 'quasipoisson')
# plot(m2, ask = FALSE)
# summary(m2)

# lets just add in country as a fixed effect here
m2a <- glm(log(Sample) ~ PeriodSamp + 
                        Paid +
                        difficulty_score +
                        comm_cat +
                        QuizTime +
                        # Team + # this has no effct on the model but fixes UK issue
                        Rainfall +
                        Temp +
                        Country,
            data = clean_data,
            family = 'quasipoisson')
plot(m2a, ask = FALSE)
# the three outliers without log(samples) are the three people with the most samples
summary(m2a)
## I think this is the model we want to go with 
deviance_explained <- (m2a$null.deviance - m2a$deviance) / m2a$null.deviance 
deviance_explained

# # removing the outliers doesn't really help
# m2a2 <- glm(Sample ~ PeriodSamp + 
#                         Paid +
#                         difficulty_score +
#                         comm_cat +
#                         QuizTime +
#                         Team +
#                         Rainfall +
#                         Temp +
#                         Country,
#             data = clean_data[!row.names(clean_data) %in% c('5480', '182', '298'),],
#             family = 'quasipoisson')
# plot(m2a2, ask = FALSE)
# # the three outliers without log(samples) are the three people with the most samples
# summary(m2a2)
# 
# # Might it be better explained with a negative binomial?
# m2b <- MASS::glm.nb(Sample ~ PeriodSamp + 
#                         Paid + 
#                         difficulty_score + 
#                         comm_cat + 
#                         QuizTime + 
#                         Team +
#                         Rainfall +
#                         Temp + 
#                         Country,
#             data = clean_data)
# plot(m2b, ask = FALSE)
# summary(m2b)
# 
# # Might it be better explained with a negative binomial?
# m3 <- MASS::glm.nb(Sample ~ scale(PeriodSamp) + 
#                         Paid + 
#                         difficulty_score + 
#                         comm_cat + 
#                         QuizTime + 
#                         Team +
#                         Rainfall +
#                         Temp, 
#                         # (1|Country),
#             data = clean_data)
# plot(m3, ask = FALSE)
# summary(m3)
# # this doesnt look so good
# 
# # try with an observation level random effect
# m4 <- glmer(Sample ~ scale(PeriodSamp) + 
#                      Paid + 
#                      difficulty_score + 
#                      comm_cat + 
#                      QuizTime + 
#                      Team +
#                      Rainfall +
#                      Temp +
#                      (1|Country) +
#                      (1|uid),
#             data = clean_data,
#             family = 'poisson')
# plot(m4)
# summary(m4)
# # this does not seem happy!

```

The results from the mixed effects model shows that a number of parameters are important. We plot the random effect which show how the countries vary in the amount of samples they generate and we plot the fixed effects. The increasing error in these i think reflects the fact that in most cases sample size decreases as values increase.